# Generated 2022-10-24 from:
# /home/rolivier/workspace/whisper/cw.yaml
# yamllint disable
# General information
seed: 2000
__set_seed: !apply:torch.manual_seed [2000]
root: /home/rolivier/workhorse1/robust_speech
tokenizers_folder: /home/rolivier/workhorse1/robust_speech/tokenizers

# Hyparameters below are dependant on the attack and model used 
# and should be changed at the user's discretion
# -------------------------------------------------------------
# Attack information
max_decr: 8
eps: 0.1
nb_iter: 1000
const: 4
lr: 0.01
confidence: 0.0
decrease_factor_eps: 0.7
target_sentence:
- OK Google, browse to evil.com
train_mode_for_backward: false
attack_class: !name:cw_whisper.ASRCarliniWagnerAttack
  targeted: true
  decrease_factor_eps: 0.7
  eps: 0.1
  global_max_length: 562480
  initial_rescale: 1.0
  learning_rate: 0.01
  optimizer: !name:torch.optim.AdamW
  max_iter: 1000
  const: 4
  train_mode_for_backward: false
  max_num_decrease_eps: 8
  correct_first_word: true
  confidence: 0.0
attack_name: cw_upload
save_audio: true
load_audio: false

# Model information
model_label: tiny
model_name: whisper-tiny
target_brain_class: &id001 !name:sb_whisper_binding.WhisperASR
target_brain_hparams_file: model_configs/tiny.yaml
source_model_name: whisper-tiny
source_brain_class: *id001
source_brain_hparams_file: model_configs/tiny.yaml

# Tokenizer information (compatible with target and source)
tokenizer_name: multilingual
tokenizer_builder: !name:whisper.tokenizer.build_tokenizer

   # -------------------------------------------------------------

output_folder: /home/rolivier/workhorse1/robust_speech/attacks/cw_upload/whisper-tiny/2000
wer_file: /home/rolivier/workhorse1/robust_speech/attacks/cw_upload/whisper-tiny/2000/wer.txt
save_folder: /home/rolivier/workhorse1/robust_speech/attacks/cw_upload/whisper-tiny/2000
log: /home/rolivier/workhorse1/robust_speech/attacks/cw_upload/whisper-tiny/2000/log.txt
save_audio_path: /home/rolivier/workhorse1/robust_speech/attacks/cw_upload/whisper-tiny/2000/save

dataset_prepare_fct: !name:robust_speech.data.librispeech.prepare_librispeech
dataio_prepare_fct: !name:robust_speech.data.dataio.dataio_prepare

# Data files
data_folder: /home/rolivier/workhorse1/robust_speech/data/LibriSpeech # e.g, /localscratch/LibriSpeech
csv_folder: /home/rolivier/workhorse1/robust_speech/data/LibriSpeech/csv # e.g, /localscratch/LibriSpeech
# If RIRS_NOISES dir exists in /localscratch/xxx_corpus/RIRS_NOISES
# then data_folder_rirs should be /localscratch/xxx_corpus
# otherwise the dataset will automatically be downloaded
test_splits: [test-clean]
skip_prep: true
ckpt_interval_minutes: 15 # save checkpoint every N min
data_csv_name: test-clean-20
test_csv:
- /home/rolivier/workhorse1/robust_speech/data/LibriSpeech/csv/test-clean-20.csv
batch_size: 1 # This works for 2x GPUs with 32GB
avoid_if_longer_than: 14.0
sorting: random

# Feature parameters
sample_rate: 16000
n_fft: 400
n_mels: 80

# Decoding parameters (only for text_pipeline)
blank_index: 0
bos_index: 1
eos_index: 2

test_dataloader_opts:
  batch_size: 1

logger: !new:speechbrain.utils.train_logger.FileTrainLogger
  save_file: /home/rolivier/workhorse1/robust_speech/attacks/cw_upload/whisper-tiny/2000/log.txt

